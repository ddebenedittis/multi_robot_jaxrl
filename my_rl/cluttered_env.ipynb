{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluttered Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from cluttered_env import ClutteredEnv, EnvParams\n",
    "\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "sys.path.insert(0, parent_dir)\n",
    "\n",
    "from purejaxrl.purejaxrl.wrappers import (\n",
    "    NormalizeVecObsEnvState,\n",
    "    ClipAction,\n",
    ")\n",
    "\n",
    "config = {\n",
    "    \"LR\": 3e-4,\n",
    "    \"NUM_ENVS\": 1024,\n",
    "    \"NUM_STEPS\": 256,\n",
    "    \"TOTAL_TIMESTEPS\": 5e7,\n",
    "    \"UPDATE_EPOCHS\": 4,\n",
    "    \"NUM_MINIBATCHES\": 32,\n",
    "    \"GAMMA\": 0.99,\n",
    "    \"GAE_LAMBDA\": 0.95,\n",
    "    \"CLIP_EPS\": 0.2,\n",
    "    \"ENT_COEF\": 0.0,\n",
    "    \"VF_COEF\": 0.5,\n",
    "    \"MAX_GRAD_NORM\": 0.5,\n",
    "    \"ACTIVATION\": \"relu\",\n",
    "    \"ANNEAL_LR\": False,\n",
    "    \"NORMALIZE_ENV\": True,\n",
    "    \"DEBUG\": True,\n",
    "}\n",
    "\n",
    "rng = jax.random.PRNGKey(0)\n",
    "rng, key_reset, key_act, key_step = jax.random.split(rng, 4)\n",
    "\n",
    "env_params = EnvParams()\n",
    "env = ClutteredEnv()\n",
    "env = ClipAction(env)\n",
    "\n",
    "# Reset the environment.\n",
    "obs, state = env.reset(key_reset, env_params)\n",
    "\n",
    "# Sample a random action.\n",
    "action = env.action_space(env_params).sample(key_act)\n",
    "\n",
    "# Perform the step transition.\n",
    "n_obs, n_state, reward, done, _ = env.step(key_step, state, action, env_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ppo_continuous_action import ActorCritic, make_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = jax.random.PRNGKey(30)\n",
    "train_jit = jax.jit(make_train(ClutteredEnv(), EnvParams(), config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "rng = jax.random.PRNGKey(42)\n",
    "t0 = time.time()\n",
    "out = jax.block_until_ready(train_jit(rng))\n",
    "print(f\"time: {time.time() - t0:.2f} s\")\n",
    "plt.plot(out[\"metrics\"][\"returned_episode_returns\"].mean(-1).reshape(-1))\n",
    "plt.xlabel(\"Update Step\")\n",
    "plt.ylabel(\"Return\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_state = out['runner_state'][0]\n",
    "env_state = out['runner_state'][1].env_state\n",
    "env_state = NormalizeVecObsEnvState(\n",
    "    mean=jnp.mean(env_state.mean, axis=0),\n",
    "    var=jnp.mean(env_state.var, axis=0),\n",
    "    count=env_state.count,\n",
    "    env_state=env_state.env_state,\n",
    ")\n",
    "\n",
    "env_params = EnvParams(max_steps_in_episode=1000)\n",
    "\n",
    "network = ActorCritic(\n",
    "    action_dim=env.action_space(env_params).shape[0], \n",
    "    activation=config[\"ACTIVATION\"]\n",
    ")\n",
    "\n",
    "def policy_inference(network, params, obs):\n",
    "    pi, _ = network.apply(params, obs)\n",
    "    return pi.mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_reset, key_step = jax.random.split(rng)\n",
    "state_seq = []\n",
    "\n",
    "obs, state = env.reset(key_reset, env_params)\n",
    "state_seq.append(state)\n",
    "\n",
    "for i in range(1000):\n",
    "    obs = (obs - env_state.mean) / jnp.sqrt(env_state.var + 1e-8)\n",
    "    \n",
    "    action = policy_inference(network, train_state.params, obs)\n",
    "    \n",
    "    rng, key_reset, key_step = jax.random.split(rng, 3)\n",
    "    obs, state, reward, done, _ = env.step(key_step, state, action, env_params)\n",
    "    \n",
    "    if done:\n",
    "        obs, state = env.reset(key_reset, env_params)\n",
    "        \n",
    "    state_seq.append(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame\n",
    "import numpy as np\n",
    "from IPython.display import display, clear_output\n",
    "from PIL import Image\n",
    "\n",
    "x = np.array([state.x for state in state_seq])\n",
    "y = np.array([state.y for state in state_seq])\n",
    "theta = np.array([state.theta for state in state_seq])\n",
    "\n",
    "target_pos = np.array([state.target_state for state in state_seq])\n",
    "obs_pos = np.array([state.obs_state[:,0:2] for state in state_seq])\n",
    "\n",
    "# Initialize pygame\n",
    "pygame.init()\n",
    "\n",
    "# Set up display\n",
    "width, height = 600, 600\n",
    "screen = pygame.Surface((width, height))  # Create an off-screen surface\n",
    "clock = pygame.time.Clock()\n",
    "\n",
    "# Basic Colors\n",
    "BG = (15, 16, 31)\n",
    "ROBOT = (255, 255, 255)\n",
    "TARGET = (0, 255, 0)\n",
    "OBS = (0, 101, 252)\n",
    "\n",
    "radius = 10\n",
    "\n",
    "# Render loop\n",
    "for i in range(1000):\n",
    "    screen.fill(BG)  # Clear screen\n",
    "    coord = (\n",
    "        int((x[i] + 10) * 600 / 20),\n",
    "        int((y[i] + 10) * 600 / 20),\n",
    "    )\n",
    "    \n",
    "    pygame.draw.circle(screen, ROBOT, coord, radius)\n",
    "    for j in range(obs_pos[i].shape[0]):\n",
    "        coord = (\n",
    "            int((obs_pos[i][j, 0] + 10) * 600 / 20),\n",
    "            int((obs_pos[i][j, 1] + 10) * 600 / 20),\n",
    "        )\n",
    "        pygame.draw.circle(screen, OBS, coord, radius)\n",
    "\n",
    "    pygame.draw.circle(\n",
    "        screen, TARGET,\n",
    "        (int((target_pos[i, 0] + 10) * 600 / 20),\n",
    "         int((target_pos[i, 1] + 10) * 600 / 20)),\n",
    "        radius\n",
    "    )\n",
    "\n",
    "    # Convert surface to image and display in Jupyter\n",
    "    arr = pygame.surfarray.array3d(screen)\n",
    "    img = Image.fromarray(np.rot90(arr))\n",
    "    clear_output(wait=True)\n",
    "    display(img)\n",
    "    # pygame.image.save(screen, f\"frames/frame_{i:04d}.png\")\n",
    "    clock.tick(50)\n",
    "\n",
    "pygame.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
